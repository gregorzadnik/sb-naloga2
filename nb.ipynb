{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# A function that receives a path to an image\n",
    "# It returns a tuple (regular_image, gray_image)\n",
    "def get_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return (image, gray)\n",
    "\n",
    "# A function that receives a path to a file with annotations and\n",
    "# returns an array of last 4 (x-center, y-center, width, height)\n",
    "def get_annotations(file_path):\n",
    "    annotations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            annotations = line.split()[-4:]\n",
    "    return [float(x) for x in annotations]\n",
    "\n",
    "# A function that receives an image and coordinates of ears and\n",
    "# draws a rectangle around them and displays the image\n",
    "def draw_ears(image, ears, annotations):\n",
    "    frame = []\n",
    "    # Draw the rectangle we get from the predictor\n",
    "    prediction_x = 0\n",
    "    prediction_y = 0\n",
    "    for (x, y, w, h) in ears:\n",
    "        start = (x,y)\n",
    "        end = (x+w, y+h)\n",
    "        prediction_x = (int(x+w/2))\n",
    "        prediction_y = int(y+h/2)\n",
    "        print(type(image))\n",
    "        frame = cv2.rectangle(image, start, end, (0,0,255), 2)\n",
    "    \n",
    "    # Draw the rectangle we get from the text file\n",
    "    # First turn the annotations into a useful value\n",
    "    h, w, _ = image.shape\n",
    "    width = int(annotations[2] * image.shape[1])\n",
    "    height = int(annotations[3] * image.shape[0])\n",
    "\n",
    "    center_x = int(annotations[0]*w)\n",
    "    center_y = int(annotations[1]*h)\n",
    "    top_left = (int(center_x - width//2), int(center_y - height//2))\n",
    "    bot_right = (int(center_x + width//2), int(center_y + height//2))\n",
    "\n",
    "    frame = cv2.rectangle(frame, top_left, bot_right, (0, 255, 0), 2)\n",
    "\n",
    "    # Compare the centers of the predictions and the annotations\n",
    "    #print(f\"Prediction center: ({prediction_x},{prediction_y})\")\n",
    "    #print(f\"Annotation center: ({center_x},{center_y})\")\n",
    "    \n",
    "    cv2.imshow('Ears', frame)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "# A function that receives the ground truth, predicted rectangle and the image and\n",
    "# returns the IoU of that prediction\n",
    "def get_iou(ground_truth, prediction):\n",
    "    # Get the coordinates of the intersection \n",
    "    x1 = max(ground_truth[0], prediction[0])\n",
    "    y1 = max(ground_truth[1], prediction[1])\n",
    "    x2 = min(ground_truth[2], prediction[2])\n",
    "    y2 = min(ground_truth[3], prediction[3])\n",
    "\n",
    "    # Compute area of intersection\n",
    "    intersection_area = max(0, x2-x1 + 1) * max(0, y2-y1 + 1)\n",
    "\n",
    "    # Compute the area of the 2 other rectangles\n",
    "    truth_area = (ground_truth[2] - ground_truth[0] + 1) * (ground_truth[3] - ground_truth[1] +1)\n",
    "    prediction_area = (prediction[2] - prediction[0] + 1) * (prediction[3] - prediction[1] +1)\n",
    "\n",
    "    #print(f\"Truth area: {truth_area}\")\n",
    "    #print(f\"Prediction area: {prediction_area}\")\n",
    "    #print(f\"Intersection area: {intersection_area}\")\n",
    "    \n",
    "    # Compute and return the IoU\n",
    "    return intersection_area / (truth_area + prediction_area - intersection_area)\n",
    "\n",
    "# A function that receives the text file coordinates, coordinates from the predictions and the image.\n",
    "# Returns 2 arrays, each containing the x1, y1, x2, y2 coordinates (in pixels) respectively\n",
    "def get_rectangle_coordinates(annotations, ears, image_shape, image):\n",
    "    prediction = [0,0,0,0]\n",
    "    ground_truth = [0,0,0,0]\n",
    "\n",
    "    for (x, y, w, h) in ears:\n",
    "        prediction[0] = x\n",
    "        prediction[1] = y\n",
    "        prediction[2] = x + w\n",
    "        prediction[3] = y + h\n",
    "\n",
    "    h, w = image_shape\n",
    "    width = int(annotations[2] * w)\n",
    "    height = int(annotations[3] * h)\n",
    "\n",
    "    center_x = int(annotations[0]*w)\n",
    "    center_y = int(annotations[1]*h)\n",
    "    top_left = (int(center_x - width//2), int(center_y - height//2))\n",
    "    bot_right = (int(center_x + width//2), int(center_y + height//2))\n",
    "\n",
    "    ground_truth[0] = top_left[0]\n",
    "    ground_truth[1] = top_left[1]\n",
    "    ground_truth[2] = bot_right[0]\n",
    "    ground_truth[3] = bot_right[1]\n",
    "\n",
    "    # Comment these 4 lines\n",
    "    frame = cv2.rectangle(image, (prediction[0], prediction[1]), (prediction[2], prediction[3]), (0,0,255), 2)\n",
    "    frame = cv2.rectangle(frame, (ground_truth[0], ground_truth[1]), (ground_truth[2], ground_truth[3]), (0,255,0), 2)\n",
    "    cv2.imshow(\"Predictions\", frame)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    return ground_truth, prediction\n",
    "\n",
    "# A function that receives a path to a directory and \n",
    "# returns all the .pngs in that folder\n",
    "def get_images(folder_path):\n",
    "    return glob(f\"{path}/*.png\")\n",
    "\n",
    "\n",
    "# A function that receives the path to an image, \n",
    "# makes the haar cascade ear prediction and compares the results to the ground truth.\n",
    "# It returns the IoU\n",
    "def compute_for_image_haar(image_path, detector_left, detector_right):\n",
    "    annotations_path = image_path[:-4] + \".txt\"\n",
    "    #print(annotations_path)\n",
    "    img, gray = get_image(image_path)\n",
    "    annotations = get_annotations(annotations_path)\n",
    "\n",
    "    # First try to detect left ears\n",
    "    ears = detector_left.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5,\n",
    "    \t\tminSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    # If left ears are not found, try right ears\n",
    "    if(len(ears) == 0):\n",
    "        ears = detector_right.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5,\n",
    "    \t\tminSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    iou = 0\n",
    "    # If we get a result, compute the IoU and return it\n",
    "    if(len(ears) > 0):\n",
    "        image_shape = (len(img), len(img[0]))\n",
    "        ground_truth, prediction = get_rectangle_coordinates(annotations, ears, image_shape)\n",
    "        iou = get_iou(ground_truth, prediction)\n",
    "        return iou\n",
    "    # If we didn't find an ear, return None so this result can be ignored\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"files\\\\test\"\n",
    "images = get_images(path)\n",
    "iou_sum = 0\n",
    "detected = 0\n",
    "\n",
    "# Iterate through all the images\n",
    "for index, image in enumerate(images):\n",
    "    print(f\"Image {index}/{len(images)}\")\n",
    "    print()\n",
    "    iou = compute_for_image_haar(image)\n",
    "    # If we get None, don't count this case\n",
    "    if(iou is not None):\n",
    "        iou_sum += iou\n",
    "        detected += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=\"files\\\\yolo5s.pt\", force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that receives yolo output and returns and array containing\n",
    "# one array of 2 points\n",
    "def restructure_yolo_data(yolo_tensor):\n",
    "    array = yolo_tensor.numpy()[0]\n",
    "    array[0] = array[0] - array[2]/2\n",
    "    array[1] = array[1] - array[3]/2\n",
    "    array = [int(x) for x in array][:4]\n",
    "    return [array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that receives the path to an image and a yolo model, \n",
    "# makes the yolo ear prediction and compares the results to the ground truth.\n",
    "# It returns the IoU\n",
    "def compute_for_image_yolo(image_path, model):\n",
    "    result = model(image_path)\n",
    "    tensor = result.xywh[0]\n",
    "    # If the model didn't find an ear, return None so this result can be ignored\n",
    "    if len(tensor) == 0:\n",
    "        return None\n",
    "    # Otherwise, convert the values we got from the model into an array if integers\n",
    "    ears = restructure_yolo_data(tensor)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_shape = (len(image), len(image[0]))\n",
    "\n",
    "    annotations_path = image_path[:-4] + \".txt\"\n",
    "    annotations = get_annotations(annotations_path)\n",
    "    ground_truth, prediction = get_rectangle_coordinates(annotations, ears, image_shape, image)\n",
    "    iou = get_iou(ground_truth, prediction)\n",
    "    print(iou)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7480503795721187\n"
     ]
    }
   ],
   "source": [
    "compute_for_image_yolo(\"files\\\\test\\\\0501.png\", yolo_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
